{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is not about learning math. Its to take a quick look at the class that stores the hyperparameters, the class that samples the training data and the functions that generate the appropriate masks. Except for the Masks, these have veru little to do with transformers, but I place them here for transparency, which might help for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching = False # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options is just a place to store all your constants, hyper parameters and directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self, batchsize=4, device=-1, epochs=20, lr=0.01, \n",
    "                 beam_width=2, max_len=20, save_path='saved/weights/model_weights'):\n",
    "        self.batchsize = batchsize\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.k = beam_width\n",
    "        self.max_len = max_len\n",
    "        self.save_path = save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if teaching:\n",
    "    \n",
    "    opt = Options(batchsize=2, device=torch.device(\"cpu\"), epochs=25, \n",
    "                  lr=0.01, beam_width=3, max_len = 25, save_path = '../saved/weights/MoveDataExample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    '''\n",
    "    patch on Torchtext's batching process that makes it more efficient\n",
    "    http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n",
    "    '''\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "                \n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.tweettokenizer = TweetTokenizer()\n",
    "            \n",
    "    def tokenize(self, sentence):\n",
    "        sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\;]\", \" \", str(sentence))\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "        sentence = self.tweettokenizer.tokenize(sentence)\n",
    "        return sentence\n",
    "    \n",
    "def num_batches(train):\n",
    "\n",
    "    for i, b in enumerate(train):\n",
    "        pass\n",
    "    \n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2datatools(path = None, tokenizer = None, opt = None):\n",
    "\n",
    "    if opt == None:\n",
    "        opt = Options()\n",
    "        opt.batchsize = 4\n",
    "        opt.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    if path == None:\n",
    "        path = 'saved/pairs.json' \n",
    "\n",
    "    if tokenizer == None:\n",
    "        tokenizer = Tokenizer()\n",
    "        \n",
    "    input_field = data.Field(lower=True, tokenize=tokenizer.tokenize)\n",
    "    output_field = data.Field(lower=True, tokenize=tokenizer.tokenize, \n",
    "                              unk_token='<unk>', init_token='<sos>', eos_token='<eos>')\n",
    "\n",
    "    fields={'listen':('listen', input_field),'reply':('reply', output_field)} \n",
    "\n",
    "    trainingset = data.TabularDataset(path, format='json', fields=fields) \n",
    "\n",
    "    input_field.build_vocab(trainingset)\n",
    "    output_field.build_vocab(trainingset)\n",
    "    training_iterator = MyIterator(trainingset, batch_size=opt.batchsize, \n",
    "                        device=opt.device, repeat=False, \n",
    "                        sort_key=lambda x: (len(x.listen), len(x.reply)), \n",
    "                        train=True, shuffle=True)\n",
    "    opt.src_pad = input_field.vocab.stoi['<pad>']\n",
    "    opt.trg_pad = output_field.vocab.stoi['<pad>']\n",
    "    return training_iterator, input_field, output_field, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if teaching:\n",
    "    data_iter, infield, outfield, opt = json2datatools(path='../saved/examplepairs.json', opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x1185944c0>>, {'<unk>': 0, '<pad>': 1, 'chloe': 2, 'you': 3, '?': 4, 'hi': 5, 'i': 6, 'are': 7, 'haha': 8, 'lol': 9, 'ok': 10, 'bye': 11, 'hello': 12, 'how': 13, 'later': 14, 'sure': 15, 'who': 16, '!': 17, 'a': 18, 'alive': 19, 'am': 20, 'any': 21, 'dont': 22, 'dunno': 23, 'go': 24, 'gotta': 25, 'ill': 26, 'joke': 27, 'know': 28, 'me': 29, 'more': 30, 'see': 31, 'talk': 32, 'tell': 33, 'to': 34, 'true': 35, 'ttyl': 36, 'yes': 37})\n",
      " ------------------------------------------------------ \n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x118594a60>>, {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3, '?': 4, 'you': 5, 'i': 6, 'joke': 7, '!': 8, ',': 9, 'a': 10, 'can': 11, 'cats': 12, 'do': 13, 'french': 14, 'hi': 15, 'how': 16, 'say': 17, 'tell': 18, 'thank': 19, 'thanks': 20, 'at': 21, 'bye': 22, 'for': 23, 'laughing': 24, 'my': 25, 'ttyl': 26, 'am': 27, 'beaucoup': 28, 'chloe': 29, 'dont': 30, 'either': 31, 'know': 32, 'meowci': 33, 'alive': 34, 'are': 35, 'have': 36, 'me': 37, 'teach': 38, 'to': 39, 'viruses': 40, 'will': 41})\n"
     ]
    }
   ],
   "source": [
    "print(infield.vocab.stoi)\n",
    "print(\" ------------------------------------------------------ \")\n",
    "print(outfield.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(data_iter): \n",
    "    src = batch.listen.transpose(0,1) # get an input ready\n",
    "    trg = batch.reply.transpose(0,1)  # get a target ready\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31,  3, 14,  2],\n",
       "        [10, 36,  2, 17]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2, 22, 26,  3],\n",
       "        [ 2, 22, 26,  3]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, opt):\n",
    "    \"Mask out subsequent positions. aka subsequent_mask\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    np_mask =  torch.from_numpy(np_mask) == 0\n",
    "    if opt.device == torch.device(\"cuda:0\"):\n",
    "      np_mask = np_mask.cuda()\n",
    "    return np_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src, trg, opt):\n",
    "    src_mask = (src != opt.src_pad).unsqueeze(-2)\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, opt)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if teaching:\n",
    "    src_mask, trg_mask = create_masks(src, trg, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]],\n",
       "\n",
       "        [[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subset_weights(whole_model, opt):\n",
    "    '''\n",
    "    This function allows you to load saved weights from a saved model that is a subset of your model\n",
    "    It looks for the named parameters that match and loads those but will not crash trying to load\n",
    "    parameters that dont have a matching name\n",
    "    '''\n",
    "    subset_model_dict = torch.load(opt.save_path)\n",
    "    whole_model_dict = whole_model.state_dict() \n",
    "    for name, param in whole_model_dict.items(): \n",
    "        if name in subset_model_dict:\n",
    "            whole_model_dict[name].copy_(subset_model_dict[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next place to study is notebooks/Elements.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
