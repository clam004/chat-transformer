{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586d55f2",
   "metadata": {},
   "source": [
    "# QKV-Attention and Masking\n",
    "\n",
    "QKV-Attention (scaled-dot-product and multi-headed attention), Masking and Positional Encoding are the main 3 concepts that in my opinion made transformers different from their precursor recurrent neural networks in 2017\n",
    "\n",
    "## Scaled Dot-Product Attention \n",
    "\n",
    "Scaled Dot-Product Attention is used inside Multi-Head Self-Attention.\n",
    "\n",
    "In the original [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) paper it is expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}\n",
    "$$\n",
    "\n",
    "Q stands for query, K for key and V for value vectors respectively. The expression inside the softmax is called the scores. You can think of Q, K and V as serving 3 distinct roles for the same sentence, sequence encoding, or sequence of vectors. Their shapes are:\n",
    "\n",
    "1. q (q_seq_len, emb_dim)\n",
    "2. k (kv_seq_len, emb_dim)\n",
    "3. v (kv_seq_len, emb_dim)\n",
    "\n",
    "\n",
    "K vectors are like V vector's representatives, interacting with vectors in Q on V's behalf. The larger the dot product between K_i and Q_j, the greater the contribution V_i will make to the new V_j. \n",
    "\n",
    "Q does not have to come from the same sequnce encoding as K and V. For example in encoder decoder transformers their is a layer sometimes called *cross attention* where Q is from one sequence and K, V is from another. The Q sequence does not need to be the same sequence length of the K-V sequence, but K and V do have to come from the same sequence. When Q, K, V come from the same sequence, this is called self-attention. The inputs to the multiheadattn function below take (batch_size, sequence_len, emb_dim) shape tensors that represent sequences in the order of the tensors that are transformed into Q, K, and V in that order.\n",
    "\n",
    "\n",
    "Decoder Encoder Attention\n",
    "`en_attn,en_scores=multiheadattn(decode,encode,encode,encodemask)`\n",
    "\n",
    "Self Attention\n",
    "`self_attn,self_scores=multiheadattn(decode,decode,decode,decodemask)` \n",
    "\n",
    "The image below depicts from left right: \n",
    "\n",
    "1. a Q tensor of shape (Q sequence_length, embedding_dimension = (2,3)\n",
    "\n",
    "2. a transposed K tensor of shape (embedding_dimension, KV sequence_length) = (3,2)\n",
    "\n",
    "3. a score tensor of shape (Q sequence_length, KV sequence_length)\n",
    "\n",
    "<img src=\"../saved/images/scorematrix.png\">\n",
    "\n",
    "In the above image, the rightmost tensor is the scores tensor. The resulting scores tensor's vertical axis (dim=0) corresponds to the sequence length of Q and the horizontal axis (dim=1) corresponds to the sequence length of K. The above image represented in code is:\n",
    "\n",
    "```python\n",
    "k_T = k.transpose(-2, -1) \n",
    "\n",
    "scores = torch.matmul(q, k_T)\n",
    "```\n",
    "PyTorch's matmul operation, performs matrix multiplication on the last two dimensions of our tensors. torch.bmm does something similar.\n",
    "\n",
    "What is not shown is that between the top image and the bottom image, the scores tensor is scaled by dividing by the square root of the embedding dimension and a softmax function normalizes each element of the Q sequence to be a probability distribution over each element of the K-V sequence. The Horizontal axis is the last dimension, thus dim = -1 is passed to `F.softmax`. \n",
    "\n",
    "```python\n",
    "scores = scores / math.sqrt(emb_dim) \n",
    "\n",
    "softscores = F.softmax(scores, dim=-1)\n",
    "```\n",
    "\n",
    "The image below depicts from left to right:\n",
    "\n",
    "1. the scaled and softmax normalized `softscores` on the left.\n",
    "\n",
    "2. a V tensor of shape (V sequence_length, embedding_dimension = (2,3)\n",
    "\n",
    "3. a re-composed Q tensor, recomposed from portions of V. This output tensor has shape (Q sequence_length, emb_dim)\n",
    "\n",
    "<img src=\"../saved/images/contextmatrix.png\">\n",
    "\n",
    "One way to interpret the first row of the score matrix is that it is saying \"rebalance the word 'Thinking' by taking 80% of the magnitude of 'Thinking' and add it to 20% of the value of 'Machines'\"\"\n",
    "\n",
    "The length of Q and K-V above happen to both be 2, (Q sequence_length, K sequence_length) = (2,2), but remember that the sequence length of Q and K-V can be different. In the line  `output = torch.matmul(scores, v)` we are essentilly performing the operations of shape <font color='red'>score matrix (q_seq_len, kv_seq_len)</font> X <font color='lightblue'>values (kv_seq_len, dim)</font> = <font color='blue'>context values (q_seq_len, dim)</font> for every sample in the batch and for every head. The final shape and sequence length of Scaled-Dot-Product Attention on Q, K V is (Q sequence_length, emb_dim) since the image represents an operation of shape: \n",
    "\n",
    "(Q sequence_length, KV sequence_length) x (KV sequence_length, emb_dim) =\n",
    "(Q sequence_length, emb_dim)\n",
    "\n",
    "The demo below uses a toy input, simulating a batch with samples, sequence length, and a embedding dimension. \n",
    "\n",
    "The entire equation is carried out in the pytorch tensor example below\n",
    "\n",
    "```python\n",
    "scores\n",
    "tensor([[[[ 0.2509,  0.0725,  0.0990],\n",
    "          [ 0.0483, -1.8634, -1.4245]]]])\n",
    "softscores\n",
    "tensor([[[[0.3710, 0.3104, 0.3187],\n",
    "          [0.7262, 0.1073, 0.1665]]]])\n",
    "output\n",
    "tensor([[[[ 0.5732,  0.4398,  0.0379,  0.4533],\n",
    "          [ 1.0041,  0.5920, -0.1833,  0.6731]]]])\n",
    "```\n",
    "\n",
    "notice that the values in softscores sum to 1.0 across\n",
    "the horizontal axis, this axis is `dim=-1` in \n",
    "`softscores = F.softmax(scores, dim=-1)` \n",
    "if you do `softscores = F.softmax(scores, dim=-2)` \n",
    "\n",
    "you will mistakenly get instead\n",
    "\n",
    "```python\n",
    "softscores\n",
    "tensor([[[[0.5505, 0.8739, 0.8211],\n",
    "          [0.4495, 0.1261, 0.1789]]]])\n",
    "```\n",
    "\n",
    "On the Y-axis are the number of words in the Q sequence. Each row is a word in Q. So the length of this dimension is the same as the number of words in Q. \n",
    "\n",
    "On the X-axis is how much each Q-word relies on each K-V word. So the length of this dimension is the same as the number of words in K or V. \n",
    "For example if the index (1,2) position is 0.7 it means that the 2nd word in Q replies 70% on the 3rd word in K-V. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8dcfe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores\n",
      "tensor([[[[ 0.2509,  0.0725,  0.0990],\n",
      "          [ 0.0483, -1.8634, -1.4245]]]])\n",
      "softscores\n",
      "tensor([[[[0.3710, 0.3104, 0.3187],\n",
      "          [0.7262, 0.1073, 0.1665]]]])\n",
      "output\n",
      "tensor([[[[ 0.5732,  0.4398,  0.0379,  0.4533],\n",
      "          [ 1.0041,  0.5920, -0.1833,  0.6731]]]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 1\n",
    "num_heads = 1\n",
    "q_seq_len = 2\n",
    "k_seq_len = 3\n",
    "v_seq_len = 3\n",
    "emb_dim = 4\n",
    "\n",
    "q = torch.randn(batch_size, num_heads, q_seq_len, emb_dim)\n",
    "\n",
    "k = torch.randn(batch_size, num_heads, k_seq_len, emb_dim)\n",
    "v = torch.randn(batch_size, num_heads, v_seq_len, emb_dim)\n",
    "\n",
    "k_T = k.transpose(-2, -1) \n",
    "\n",
    "scores = torch.matmul(q, k_T) / math.sqrt(emb_dim) \n",
    "\n",
    "print('scores')\n",
    "print(scores)\n",
    "\n",
    "softscores = F.softmax(scores, dim=-1)\n",
    "\n",
    "print('softscores')\n",
    "print(softscores)\n",
    "\n",
    "output = torch.matmul(softscores, v)\n",
    "\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741eb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(\n",
    "    q, k, v, dim, \n",
    "    mask=None, dropout=None, explain=False\n",
    "    ):\n",
    "\n",
    "    ''' Function that performs scaled dot product attention on\n",
    "    tensors Q, K and V. each of shape:\n",
    "    \n",
    "    (batch_size, num_heads, sequence_length, dim)\n",
    "    \n",
    "    where k and v are expected to have the same sequence_length\n",
    "    but q and k do not need to have the same sequence length.\n",
    "    \n",
    "    matrix multiplication is done using the last two dimensions of \n",
    "    the input tensors:\n",
    "    \n",
    "    (batch_size, num_heads, q_seq_len, dim) \n",
    "    X \n",
    "    (batch_size, num_heads, dim, k_seq_len)\n",
    "    = \n",
    "    (batch_size, num_heads, q_seq_len, k_seq_len)\n",
    "    '''\n",
    "\n",
    "    k = k.transpose(-2, -1) \n",
    "\n",
    "    if explain: print('q, k', q.shape, k.shape)\n",
    "\n",
    "    scores = torch.matmul(q, k) / math.sqrt(dim)  \n",
    "\n",
    "    if explain: print('scores.shape', scores.shape)\n",
    "\n",
    "    if mask is not None:\n",
    "\n",
    "        mask = mask.unsqueeze(1)\n",
    "        if explain: print('mask.shape', mask.shape)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9) \n",
    "\n",
    "    softscores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None: softscores = dropout(softscores)\n",
    "\n",
    "    #(batch_size,num_heads,q_seq_len, k_seq_len)\n",
    "    #X\n",
    "    #(batch_size,num_heads,v_seq_len,dim_v)\n",
    "    \n",
    "    output = torch.matmul(softscores, v)\n",
    "\n",
    "    # output (batch_size, num_heads, q_seq_len, dim_v)\n",
    "    return output, scores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98092b97",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention\n",
    "\n",
    "The `MultiHeadAttention()` module takes in a sequence of vectors and outputs a sequence of vectors of the same shape. What it does to the sequence is modify each vector in such a way that it takes into account, to different degrees, the other vectors in the sequence. \n",
    "\n",
    "Consider the all the different meanings of the word \"date\", spelled the same way\n",
    "\n",
    "- Her favorite fruit is a date.\n",
    "- Carson took Vicki out on a date.\n",
    "- Not to date myself, but I remember listening to cassette tapes.\n",
    "- 10/26/19, thats the date of our wedding\n",
    "\n",
    "The only way for you to know which of these meanings the word \"date\" is being used as, is by using the the other words in the sentence, ie the context. \n",
    "\n",
    "For every word in the sentence, the authors of the Transformer came up with the \"attention head\". Here is one example from the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) of how one head is using the other words to modify the representation of the word \"it's\"\n",
    "\n",
    "<img src=\"../saved/images/onehead.png\">\n",
    "\n",
    "Several heads are used, to allow different types of attention, the image below shows 3 types of attention for different relationships between words \"who\", \" Did what?\" and \"To whom?\"\n",
    "\n",
    "<img src=\"../saved/images/multihead.png\">\n",
    "\n",
    "Each attention head works the same way, by transforming each vector in the sequence into another vector space. Those vectors are q, k and v, meaning the query, key and value vectors. Where the dot product between the \"it's\" query vector and the \"law\" key vector is proportional to the strength of the attention between \"it's\" and \"law\"\n",
    "\n",
    "[Jay Alammar](http://jalammar.github.io/illustrated-transformer/) does an excellent job in illustrating this in in his blog:\n",
    "\n",
    "Alammar, Jay (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "He uses the example source sequence <font color='green'>\"Thinking Machines\"</font> represented by the 2x4 matrix <font color='green'>X</font>. 2x4 because there are 2 words, each word represented by a 4 dimensional vector. The q, k and v vectors are each 3 dimensional. \n",
    "\n",
    "Going from x to q occurs by doing a matrix multiplication. If X is shape (2x4), you can use a matrix W of shape (4x3) to transform X to a matrix <font color='purple'>Q</font> of shape (2x3) Each row q is for one of the 2 words. Instead of running a for loop through all the heads, it is better to \"vectorize\" your operations for computational efficiency, by concatenating all your W's together. In the example below we have 3 heads, so instead of doing 3 seprate matrix multiplications with a transofrmation matrix (4x3) we use one large matrix shape (4x9) and slice our heads out of the longer matrix later\n",
    "\n",
    "<img src=\"../saved/images/qkvmatrix.png\">\n",
    "\n",
    "in the code below, the operation in the image is represented by the code \n",
    "\n",
    "`q = self.q_linear(q)`\n",
    "\n",
    "`k = self.k_linear(k)`\n",
    "\n",
    "`v = self.v_linear(v)`\n",
    "\n",
    "In the forward method of MultiHeadAttention above, the `view()` and `transpose()` operations serve to [re-arrange](https://discuss.pytorch.org/t/tensor-view-is-misleading/20553) the matrices q, k, into the shape **(batch_size, num_heads ,seq_len, dim_qk)**\n",
    "\n",
    "`concat=attn.transpose(1,2).contiguous().view(batch_size,-1,self.dim_k*self.num_heads)` reshapes the tensor by concatenating all the values from each head to make a tensor shape (batch size, sequence length, dim_v*num_heads)\n",
    "\n",
    "`output = self.out(concat)` uses a matrix multiplication to tranform each dim_v word vector to one of the same size as the embedding dimensions. The result is that the shape of the attention output is the same as the input. \n",
    "(batch size, sequence length, embedding dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27373ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, emb_dim, dim_k = None, dropout = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_k = dim_k if dim_k else emb_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q_linear = nn.Linear(emb_dim,self.dim_k*num_heads)\n",
    "        self.k_linear = nn.Linear(emb_dim,self.dim_k*num_heads)\n",
    "        self.v_linear = nn.Linear(emb_dim,self.dim_k*num_heads)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(self.dim_k*num_heads,emb_dim)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None, explain=False):\n",
    "        '''\n",
    "        inputs:\n",
    "            q has shape (batch size, q_sequence length, embedding dimensions)\n",
    "            k,v are shape (batch size, kv_sequence length, embedding dimensions)\n",
    "            source_mask of shape (batch size, 1, kv_sequence length)\n",
    "            \n",
    "        outputs: sequence of vectors, re-represented using attention\n",
    "            shape (batch size, q_sequence length, embedding dimensions)\n",
    "        use:\n",
    "            The encoder layer places the same source vector sequence into q, k, v \n",
    "            and source_mask into mask.\n",
    "            \n",
    "            The decoder layer uses this twice, once with decoder inputs as q, k, v \n",
    "            and target mask as mask. then with decoder inputs as q, encoder outputs\n",
    "            as k, v and source mask as mask\n",
    "        '''\n",
    "        # k,q,v are each shape (batch size, sequence length, dim_k * num_heads)\n",
    "        \n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "        \n",
    "        if explain: print(\"(batch size, sequence length, dim_k * num_heads)\", k.shape)\n",
    "            \n",
    "        # k,q,v are each shape (batch size, sequence length, num_heads, dim_k)\n",
    "        \n",
    "        k = k.view(batch_size, -1, self.num_heads, self.dim_k)\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.dim_k)\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.dim_k)\n",
    "        \n",
    "        # transpose to shape (batch_size, num_heads, sequence length, dim_k)\n",
    "        \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        if explain: print(\"(batch_size,num_heads,qk_seq_length,dim_k)\", k.shape)\n",
    "        if explain: print(\"(batch_size,num_heads,v_seq_length,dim_k)\", v.shape)\n",
    "            \n",
    "        # calculate attention using function we will define next\n",
    "        \n",
    "        attn, scores = dot_product_attention(q, k, v, self.dim_k, mask, self.dropout, explain)\n",
    "        \n",
    "        if explain: print(\"attn(batch_size,num_heads,seq_length,dim_k)\", attn.shape)\n",
    "            \n",
    "        # concatenate heads, dim_k = dim_v \n",
    "        \n",
    "        concat = attn.transpose(1,2).contiguous().view(batch_size,-1,self.dim_k*self.num_heads)\n",
    "        \n",
    "        if explain: print(\"concat.shape\", concat.shape)\n",
    "            \n",
    "        # put through final linear layer\n",
    "        \n",
    "        output = self.out(concat)\n",
    "        \n",
    "        if explain: print(\"MultiHeadAttention output.shape\", output.shape)\n",
    "            \n",
    "        return output, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef07cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vectors shape\n",
      " torch.Size([2, 3, 5])\n",
      "---------------------------------------\n",
      "input_mask\n",
      " tensor([[[ True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "input_vectors = np.asarray([ \n",
    "[[0.0, 0.0, 0.1, 0.2, 0.3],[0.0, 0.1, 0.2, 0.3, 0.4],[0.0, 0.2, 0.3, 0.4, 0.5]], \n",
    "[[0.0, 0.1, 0.2, 0.3, 0.4],[0.0, 0.2, 0.3, 0.4, 0.5],[0.0, 0.3, 0.4, 0.5, 0.6]]  \n",
    "])\n",
    "\n",
    "input_mask = torch.from_numpy(np.ones((2,1,3)) == 1)\n",
    "input_mask[0,0,-1] = False\n",
    "input_vectors = torch.from_numpy(input_vectors).float()\n",
    "\n",
    "print('input_vectors shape\\n', input_vectors.shape)\n",
    "print('---------------------------------------')\n",
    "print('input_mask\\n', input_mask, input_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e5bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(batch size, sequence length, dim_k * num_heads) torch.Size([2, 3, 24])\n",
      "(batch_size,num_heads,qk_seq_length,dim_k) torch.Size([2, 6, 3, 4])\n",
      "(batch_size,num_heads,v_seq_length,dim_k) torch.Size([2, 6, 3, 4])\n",
      "q, k torch.Size([2, 6, 3, 4]) torch.Size([2, 6, 4, 3])\n",
      "scores.shape torch.Size([2, 6, 3, 3])\n",
      "mask.shape torch.Size([2, 1, 1, 3])\n",
      "attn(batch_size,num_heads,seq_length,dim_k) torch.Size([2, 6, 3, 4])\n",
      "concat.shape torch.Size([2, 3, 24])\n",
      "MultiHeadAttention output.shape torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "multiattention = MultiHeadAttention(num_heads=6,emb_dim=5,dim_k=4,dropout=0.0)\n",
    "output, scores = multiattention(input_vectors,input_vectors,input_vectors,input_mask,explain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec2bd0",
   "metadata": {},
   "source": [
    "## Masks and Softmax\n",
    "The last thing to cover in the `MultiHeadAttention()` function is the way the mask is applied to the attention scores and the way the softmax is applied to the attention scores. \n",
    "\n",
    "In intuitive words, imagine that the first sample is `thinking machines <pad>`. We dont need to incorporate the token `<pad>` into the representation of \"thinking\" or \"machines\" so our mask for this sequence is `[ True,  True, False]`.\n",
    "In the case of the decoder output, the mask depends on what step in the output we are in. Each output can only attend to the previous outputs so the mask will look something like `[ True]` for the first step and `[ True,  True, True, False]` for the 3rd step etc. This is more relevant for training but not for inference. \n",
    "\n",
    "The mask is reshaped into (batch size, 1, 1, sequence length) in the case of the source mask, and (batch size, 1, sequence length, sequence length) in the case of the target mask.\n",
    "\n",
    "This is [broadcasted](https://medium.com/ai%C2%B3-theory-practice-business/understanding-broadcasting-in-pytorch-ca9e9533f05f) across the scores which are of shape (batch size, number of heads, sequence length, sequence length). Broadcasting will apply the mask across each row of the (sequence length x sequence length) matrix. Think of each row as the being assigned to each token. The first row is for the token \"thinking\". If this row looks something like `[2.0, 0.1, -100]`, it means that the word \"thinking\" should be defined mostly by it's own token `2.0`, alittle by the word \"machines\" `0.1`, and practically not at all by the padding token `-100`. The code `scores = scores.masked_fill(mask == 0, -1e9)` takes the very negative value, `-1e9` and replaces the score values at equivalent position where the mask has `False` or `0` as it's element, with this very negative value `-1.0000e+09`. \n",
    "\n",
    "`softscores = F.softmax(scores, dim=-1)` will then rebalance each row such that they all sum to 1.0, `dim=-1` means the last dimension. Think of a matrix that has 2 rows and 3 columns, it is a 2 x 3 matrix. If you softmax across the last dimension of size 3, you are making all 3 of those sum to 1.0 \n",
    "\n",
    "In the cell below we demonstrate what we have discussed by running the attention function without the mask, getting the unmasked score and applying the mask to the score step by step for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d40ef63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True, False]],\n",
      "\n",
      "        [[ True,  True,  True]]]) torch.Size([2, 1, 3]) torch.Size([2, 6, 3, 3])\n",
      "tensor([[-0.0193, -0.0198, -0.0204],\n",
      "        [-0.0266, -0.0269, -0.0272],\n",
      "        [-0.0339, -0.0339, -0.0339]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.0269, -0.0272, -0.0274],\n",
      "        [-0.0339, -0.0339, -0.0338],\n",
      "        [-0.0409, -0.0406, -0.0402]], grad_fn=<SliceBackward0>)\n",
      "------------------------------------------------------\n",
      "tensor([[[[ True,  True, False]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True,  True]]]]) torch.Size([2, 1, 1, 3]) torch.Size([2, 6, 3, 3])\n",
      "tensor([[-1.9260e-02, -1.9849e-02, -1.0000e+09],\n",
      "        [-2.6583e-02, -2.6866e-02, -1.0000e+09],\n",
      "        [-3.3905e-02, -3.3883e-02, -1.0000e+09]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.0269, -0.0272, -0.0274],\n",
      "        [-0.0339, -0.0339, -0.0338],\n",
      "        [-0.0409, -0.0406, -0.0402]], grad_fn=<SliceBackward0>)\n",
      "------------------------------------------------------\n",
      "tensor([[0.5001, 0.4999, 0.0000],\n",
      "        [0.5001, 0.4999, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000]], grad_fn=<SliceBackward0>)\n",
      "tensor([[0.3334, 0.3333, 0.3332],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3332, 0.3333, 0.3334]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, scores = multiattention(input_vectors,input_vectors,input_vectors,explain=False)\n",
    "print(input_mask, input_mask.shape, scores.shape)\n",
    "print(scores[0,0,:,:])\n",
    "print(scores[1,0,:,:])\n",
    "print(\"------------------------------------------------------\")\n",
    "mask = input_mask.unsqueeze(1)\n",
    "print(mask, mask.shape, scores.shape)\n",
    "scores = scores.masked_fill(mask == 0, -1e9) \n",
    "print(scores[0,0,:,:])\n",
    "print(scores[1,0,:,:])\n",
    "print(\"------------------------------------------------------\")\n",
    "softscores = F.softmax(scores, dim=-1)\n",
    "print(softscores[0,0,:,:])\n",
    "print(softscores[1,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbe8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
