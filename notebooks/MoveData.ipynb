{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, opt):\n",
    "    \"Mask out subsequent positions. aka subsequent_mask\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    np_mask =  torch.from_numpy(np_mask) == 0\n",
    "    if opt.device == torch.device(\"cuda:0\"):\n",
    "      np_mask = np_mask.cuda()\n",
    "    return np_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src, trg, opt):\n",
    "    src_mask = (src != opt.src_pad).unsqueeze(-2)\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, opt)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    '''\n",
    "    patch on Torchtext's batching process that makes it more efficient\n",
    "    http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n",
    "    '''\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_batches(train):\n",
    "\n",
    "    for i, b in enumerate(train):\n",
    "        pass\n",
    "    \n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.tweettokenizer = TweetTokenizer()\n",
    "            \n",
    "    def tokenize(self, sentence):\n",
    "        sentence = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\;]\", \" \", str(sentence))\n",
    "        #r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "        sentence = self.tweettokenizer.tokenize(sentence)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2datatools(path = None, tokenizer = None, opt = None):\n",
    "\n",
    "    if opt == None:\n",
    "        opt = Options()\n",
    "        opt.batchsize = 4\n",
    "        opt.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    if path == None:\n",
    "        path = 'saved/pairs.json' \n",
    "\n",
    "    if tokenizer == None:\n",
    "        tokenizer = Tokenizer()\n",
    "        \n",
    "    input_field = data.Field(lower=True, tokenize=tokenizer.tokenize)\n",
    "    output_field = data.Field(lower=True, tokenize=tokenizer.tokenize, \n",
    "                              unk_token='<unk>', init_token='<sos>', eos_token='<eos>')\n",
    "\n",
    "    fields={'listen':('listen', input_field),'reply':('reply', output_field)} \n",
    "\n",
    "    trainingset = data.TabularDataset(path, format='json', fields=fields) \n",
    "\n",
    "    input_field.build_vocab(trainingset)\n",
    "    output_field.build_vocab(trainingset)\n",
    "    training_iterator = MyIterator(trainingset, batch_size=opt.batchsize, \n",
    "                        device=opt.device, repeat=False, \n",
    "                        sort_key=lambda x: (len(x.listen), len(x.reply)), \n",
    "                        train=True, shuffle=True)\n",
    "    opt.src_pad = input_field.vocab.stoi['<pad>']\n",
    "    opt.trg_pad = output_field.vocab.stoi['<pad>']\n",
    "    return training_iterator, input_field, output_field, opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self, batchsize=4, device=-1, epochs=20, lr=0.01, \n",
    "                 beam_width=2, max_len=20, save_path='saved/weights/model_weights'):\n",
    "        self.batchsize = batchsize\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.k = beam_width\n",
    "        self.max_len = max_len\n",
    "        self.save_path = save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subset_weights(whole_model, opt):\n",
    "    '''\n",
    "    This function allows you to load saved weights from a saved model that is a subset of your model\n",
    "    It looks for the named parameters that match and loads those but will not crash trying to load\n",
    "    parameters that dont have a matching name\n",
    "    '''\n",
    "    subset_model_dict = torch.load(opt.save_path)\n",
    "    whole_model_dict = whole_model.state_dict() \n",
    "    for name, param in whole_model_dict.items(): \n",
    "        if name in subset_model_dict:\n",
    "            whole_model_dict[name].copy_(subset_model_dict[name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
