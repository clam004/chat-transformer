{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Elements.ipynb\n",
      "importing Jupyter notebook from MoveData.ipynb\n"
     ]
    }
   ],
   "source": [
    "import math, copy, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import import_ipynb\n",
    "from Elements import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder \n",
    "\n",
    "Compare this diagram from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) to the code in the cells below. \n",
    "\n",
    "All the subcomponents of the `EncoderLayer()` are thoroughly explained in Elements.ipynb\n",
    "\n",
    "the inputs to the encoder are sequences of integers shaped (batch size, sequence length)\n",
    "\n",
    "the output are sequences of vectors shaped (batch size, sequence length, embedding dimensions)\n",
    "\n",
    "<img src=\"../saved/images/encoderchart.png\">\n",
    "\n",
    "The diagram above mirrors the code below. As you follow the diagram upwards, you can see the data pass through the same modules as you move down the lines of code. `x` in our code is the sequence of tokens, the black line, moving through the diagram.\n",
    "\n",
    "The box labeled Nx is the `EncoderLayer` function below, that is repeated an arbitrary Nx number of times (6 in the paper). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(emb_dim)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.attn = MultiHeadAttention(heads, emb_dim, dropout=dropout)\n",
    "        self.norm_2 = Norm(emb_dim)\n",
    "        self.ff = FeedForward(emb_dim, dropout=dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, vector_sequence, mask):\n",
    "        '''\n",
    "        input:\n",
    "            vector_sequence of shape (batch size, sequence length, embedding dimensions)\n",
    "            source_mask (mask over input sequence) of shape (batch size, 1, sequence length)\n",
    "        output: sequence of vectors after embedding, postional encoding, attention and normalization\n",
    "            shape (batch size, sequence length, embedding dimensions)\n",
    "        '''\n",
    "        x2 = self.norm_1(vector_sequence)\n",
    "        x2_attn, x2_scores = self.attn(x2,x2,x2,mask)\n",
    "        vector_sequence = vector_sequence + self.dropout_1(x2_attn)\n",
    "        x2 = self.norm_2(vector_sequence)\n",
    "        vector_sequence = vector_sequence + self.dropout_2(self.ff(x2))\n",
    "        return vector_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward function of `Encoder` you can see that just as in the diagram, the embedding occurs first `x = self.embed(source_sequence)` followed by positional encoding `x = self.pe(x)`. Then the `EncoderLayer(emb_dim, heads, dropout)` is repeated an `n_layers` number of times. \n",
    "\n",
    "Within the gray box in the diagram you can see the first step is the Multi-Head Attention. In our implementation we do a normalization first. But the the data splits into 3 arrows before going into Multi-Head Attention, just like we use the `x2` three times in `x2_attn, x2_scores = self.attn(x2,x2,x2,mask)`. This signifies that we used the same sequence of vectors for the generation of `num_heads` number of query `q`, key `k` and  value `v` vectors. \n",
    "\n",
    "Another minor difference is the use of dropout. Dropout is the random zeroing out of certain neurons or activations. The line above that reads `x = x + self.dropout_1(x2_attn)` could be written `x = x + x2_attn` to make it resemble the residual connection in the diagram. The line in the diagram that branches and goes around the Multi-Head Attention layer and into the \"Add & Norm\" module, is the residual. In english, it means that the input to the norm layer is the sum of x after it passes through the Multi-Head Attention with a copy of x that has not passed through the Multi-Head Attention.\n",
    "\n",
    "In the same way, the output of the feed forward layer `self.ff(x2)` is added to itself before normalized again. This is the 2nd residual in the diagram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_layers, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = Embedder(vocab_size, emb_dim)\n",
    "        self.pe = PositionalEncoder(emb_dim, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(emb_dim, heads, dropout), n_layers)\n",
    "        self.norm = Norm(emb_dim)\n",
    "    def forward(self, source_sequence, source_mask):\n",
    "        '''\n",
    "        input:\n",
    "            source_sequence (sequence of source tokens) of shape (batch size, sequence length)\n",
    "            source_mask (mask over input sequence) of shape (batch size, 1, sequence length)\n",
    "        output: sequence of vectors after embedding, postional encoding, attention and normalization\n",
    "            shape (batch size, sequence length, embedding dimensions)\n",
    "        '''\n",
    "        vector_sequence = self.embed(source_sequence)\n",
    "        vector_sequence = self.pe(vector_sequence)\n",
    "        for i in range(self.n_layers):\n",
    "            vector_sequence = self.layers[i](vector_sequence, source_mask)\n",
    "        vector_sequence = self.norm(vector_sequence)\n",
    "        return vector_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of type and shape of input and output to Encoder \n",
    "\n",
    "if False:\n",
    "    encoder = Encoder(vocab_size=3, emb_dim=4, n_layers=1, heads=2, dropout=0)\n",
    "    source_sequence = torch.from_numpy(np.asarray([0,1,2])).unsqueeze(0)\n",
    "    input_mask = (source_sequence != 4).unsqueeze(-2)\n",
    "    encoding = encoder(source_sequence, input_mask)\n",
    "    print(\"encoding.shape\",encoding.shape)\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(\"encoding\",encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this diagram from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) to the code in the cells below. \n",
    "\n",
    "<img src=\"../saved/images/decoderchart.png\">\n",
    "\n",
    "The portion of the diagram within the grey box labeled \"Nx\" corresponds to the `DecoderLayer()` below. The data passes through `n_layers` number of `DecoderLayer()` 's as indicated by the `n_layers` argument in `Decoder()`.\n",
    "The process of embedding and postional encoding is the same as the analogous operations in the Encoder section. What is different in the Decoder are the 2 ways in which the attention is applied. \n",
    "\n",
    "In the diagram the \"Masked Multi-Head Attention\" is the box that corresponds to the line of code `self_attn, self_scores = self.attn_1(de_nrm, de_nrm, de_nrm, trg_mask)#Self Attention `. This is the self attention which attends to what th decoder has outputted so far `de_out (decoder ouputs so far)`. An intuitive explaination is that this attention prevents the network from stuttering \"I see see you now\" or from skipping \"I you now\", by attending to all the words already spoken at the time of generating a new word. \n",
    "\n",
    "The box in the diagram \"Multi-Head Attention\" is similar to the encoder attention used the encoder that attends to the input sequence. There is a small difference though. Take a look at the way this attention is used:\n",
    "\n",
    "`en_attn, en_scores = self.attn_2(de_nrm, en_out, en_out, src_mask)#Encoder Attention`\n",
    "\n",
    "the `q, k, v` sequences are `de_nrm, en_out, en_out` respectively. This is because in this case, we are re-representing our `de_out (decoder ouputs so far)` in the context of our `en_out (encoder output)`. The `q` sequence of vectors are the vectors you are trying to re-represent. `q` for query is like the question. If the `q` for the word \"she\" and the `k` for the word \"chloe\" together produce a large softmax score, it means that the pronoun \"she\" refers to \"chloe\" rather than some other word, like \"a\" or \"bowl\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(emb_dim)\n",
    "        self.norm_2 = Norm(emb_dim)\n",
    "        self.norm_3 = Norm(emb_dim)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, emb_dim, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttention(heads, emb_dim, dropout=dropout)\n",
    "        self.ff = FeedForward(emb_dim, dropout=dropout)\n",
    "\n",
    "    def forward(self, de_out, de_mask, en_out, en_mask):\n",
    "        '''\n",
    "        inputs:\n",
    "            de_out - decoder ouputs so far (batch size, output sequence length, embedding dimensions)\n",
    "            de_mask (batch size, output sequence length, output sequence length)\n",
    "            en_out - encoder output (batch size, input sequence length, embedding dimensions)\n",
    "            en_mask (batch size, 1, input sequence length)\n",
    "        ouputs:\n",
    "            de_out (next decoder output) (batch size, output sequence length, embedding dimensions)\n",
    "        '''\n",
    "        de_nrm = self.norm_1(de_out)\n",
    "        #Self Attention \n",
    "        self_attn, self_scores = self.attn_1(de_nrm, de_nrm, de_nrm, de_mask)\n",
    "        de_out = de_out + self.dropout_1(self_attn)\n",
    "        de_nrm = self.norm_2(de_out)\n",
    "        #DecoderEncoder Attention\n",
    "        en_attn, en_scores = self.attn_2(de_nrm, en_out, en_out, en_mask) \n",
    "        de_out = de_out + self.dropout_2(en_attn)\n",
    "        de_nrm = self.norm_3(de_out)\n",
    "        de_out = de_out + self.dropout_3(self.ff(de_nrm))\n",
    "        return de_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    If your target sequence is `see` `ya` and you want to train on the entire \n",
    "    sequence against the target, you would use `<sos>` `see`  `ya`\n",
    "    as the de_out (decoder ouputs so far) and compare the \n",
    "    output de_out (next decoder output) `see` `ya` `<eos>` \n",
    "    as the target in the loss function. The inclusion of the `<sos>`\n",
    "    for the (decoder ouputs so far) and `<eos>` for the \n",
    "    '''\n",
    "    def __init__(self, vocab_size, emb_dim, n_layers, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = Embedder(vocab_size, emb_dim)\n",
    "        self.pe = PositionalEncoder(emb_dim, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(emb_dim, heads, dropout), n_layers)\n",
    "        self.norm = Norm(emb_dim)\n",
    "    def forward(self, de_toks, de_mask, en_vecs, en_mask):\n",
    "        '''\n",
    "        inputs:\n",
    "            de_toks - decoder ouputs so far (batch size, output sequence length)\n",
    "            de_mask (batch size, output sequence length, output sequence length)\n",
    "            en_vecs - encoder output (batch size, input sequence length, embedding dimensions)\n",
    "            en_mask (batch size, 1, input sequence length)\n",
    "        outputs:\n",
    "            de_vecs - next decoder output (batch size, output sequence length, embedding dimensions)\n",
    "\n",
    "        '''\n",
    "        x = self.embed(de_toks)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x, de_mask, en_vecs, en_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now learned the Two main subcomponents of the Transformer. The last component of the transformer is taking these the de_out (next decoder output) and using this representation of the Transformers response and tranlating those vectors back into word tokens to generate the replay. Go to Talk.ipynb for the next lesson\n",
    "\n",
    "## How can I help you or get help from you?\n",
    "\n",
    "[Support *ChloeRobotics* on Patreon and send us a message](https://www.patreon.com/chloerobotics)\n",
    "\n",
    "## Questions?\n",
    "\n",
    "email chloe.the.robot [at] gmail [dot] com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
